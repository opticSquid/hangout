services:
  # * layer 0 components - no dependencies
  # main database
  geo-postgres:
    image: postgis/postgis:17-master
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.1
          memory: 100M
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - geopostgresData:/var/lib/postgresql/data

  # analytics database
  clickhouse:
    image: clickhouse:lts-jammy
    deploy:
      replicas: 0
      resources:
        limits:
          cpus: 0.1
          memory: 900M
    ports:
      - 18123:8123
      - 19000:9000
    environment:
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    volumes:
      - clickHouseData:/var/lib/clickhouse/
      - clickHouseLogs:/var/log/clickhouse-server/

  # object storage
  minio:
    image: minio/minio:RELEASE.2025-05-24T17-08-30Z
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.2
          memory: 350M
    command: server /data --console-address ":9001"
    ports:
      - 9000:9000
      - 9001:9001
    volumes:
      - minioData:/data
    environment:
      - MINIO_ROOT_USER=ROOTUSER
      - MINIO_ROOT_PASSWORD=${MINIO_PASSWORD}

  # metrics and alerts
  prometheus:
    image: bitnami/prometheus:3.4.0
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.1
          memory: 180M
    # required for calling local servers from prometheus
    extra_hosts: ["host.docker.internal:host-gateway"]
    command:
      - --enable-feature=exemplar-storage
      - --enable-feature=native-histograms
      - --config.file=/etc/prometheus/prometheus.yml
      - --web.enable-remote-write-receiver
      - --storage.tsdb.retention.time=1h
    volumes:
      - ./docker/prometheus/prometheus-compose.yml:/etc/prometheus/prometheus.yml:ro
      - prometheusData:/prometheus
    ports:
      - "9090:9090"

  # logs
  loki:
    image: grafana/loki:main-44b2c27
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.1
          memory: 125M
    command: ["-config.file=/etc/loki/local-config.yaml"]
    ports:
      - "3100:3100" # http log recieve port
    volumes:
      - ./docker/loki/loki-local.yaml:/etc/loki/local-config.yaml:rw
      - lokiData:/loki
    environment:
      - JAEGER_AGENT_HOST=tempo
      - JAEGER_ENDPOINT=http://tempo:14268/api/traces # send traces to Tempo
      - JAEGER_SAMPLER_TYPE=const
      - JAEGER_SAMPLER_PARAM=1

  # traces
  tempo:
    image: grafana/tempo:main-b9a611e
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.1
          memory: 180M
    command: ["-config.file=/etc/tempo.yaml"]
    volumes:
      - ./docker/tempo/tempo-local.yaml:/etc/tempo.yaml:rw
      - tempoData:/tmp

  # * layer 1 components - dependent on layer 0 compoenents
  # container monitoring
  cadvisor:
    image: ghcr.io/google/cadvisor:v0.53.0
    privileged: true
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    devices:
      - /dev/kmsg
    depends_on:
      - prometheus

  # dashboard
  grafana:
    image: grafana/grafana:12.0.1
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.2
          memory: 350M
    volumes:
      - grafanaData:/var/lib/grafana
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
      - tempo
      - loki

  # observability data collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.127.0
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.3
          memory: 300M
    command:
      - --config=/etc/otelcol-contrib/config.yaml
    volumes:
      - ./docker/collector/otel-collector-compose.yaml:/etc/otelcol-contrib/config.yaml
    ports:
      - "4318:4318" # OTLP HTTP receiver
      - "4317:4317" # OTLP GRPCreciever
      - "8888:8888" # Prometheus metrics exposed by the Collector
      - "8889:8889" # Prometheus exporter metrics
    depends_on:
      - prometheus
      - loki
      - tempo

  # * layer 2 components - dependent on layer 0,1 components
  # event bus
  kafka:
    image: bitnami/kafka:4.0.0
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.5
          memory: 600M
    ports:
      - "9092:9092"
    volumes:
      - kafkaData:/bitnami/kafka
      - ./docker/kafka/jmx-exporter.yaml:/opt/bitnami/kafka/config/jmx-exporter.yaml
      - ./docker/kafka/jmx_prometheus_javaagent-1.3.0.jar:/opt/bitnami/kafka/libs/jmx_prometheus_javaagent-1.3.0.jar
    environment:
      # KRaft settings
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      # Listeners
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,LISTENER_DOCKER_INTERNAL://:19091
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092,LISTENER_DOCKER_INTERNAL://kafka:19091
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,LISTENER_DOCKER_INTERNAL:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=LISTENER_DOCKER_INTERNAL
      # Kafka Broker settings
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_CFG_LOG_RETENTION_HOURS=1
      # Consider setting default replication factor and partitions for a single node
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=1
      - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=1 # For auto-created topics
      - KAFKA_CFG_NUM_PARTITIONS=20 # For auto-created topics
        # Enable Prometheus JMX Exporter via Bitnami specific env vars
      - KAFKA_ENABLE_PROMETHEUS_METRICS=true
      - KAFKA_PROMETHEUS_METRICS_PORT_NUMBER=9095 # Port Kafka will expose metrics on for Prometheus
      - KAFKA_JMX_PORT=9095
      - KAFKA_JMX_HOSTNAME=0.0.0.0
      - KAFKA_JMX_ENABLE=true
      - KAFKA_OPTS=-javaagent:/opt/bitnami/kafka/libs/jmx_prometheus_javaagent-1.3.0.jar=9095:/opt/bitnami/kafka/config/jmx-exporter.yaml
    depends_on:
      - otel-collector

  # Temporary access to content
  content-delivery-api:
    image: ghcr.io/opticsquid/hangout-content-delivery-api:v1.2.0
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.2
          memory: 50M
    environment:
      - AWS_ACCESS_KEY_ID=
      - AWS_SECRET_ACCESS_KEY=
      - HCDA_APPLICATION_NAME=content-delivery-api
      - HCDA_AWS_REGION=ap-south-1
      - HCDA_AWS_IMAGE_S3_BUCKET=hangout-profile-photo
      - HCDA_AWS_IMAGE_S3_EXPIRATION-DURATION-SECONDS=30
      - HCDA_AWS_VIDEO_CLOUDFRONT_PRIVATE-KEY-PATH=/mnt/certs/private_key.pem
      - HCDA_AWS_VIDEO_CLOUDFRONT_PUBLIC-KEY-ID=K2VKY8IQDVEEJD
      - HCDA_AWS_VIDEO_CLOUDFRONT_DOMAIN=cdn.hangoutsb.in
      - HCDA_AWS_VIDEO_CLOUDFRONT_EXPIRATION-DURATION-SECONDS=300
      - HCDA_COOKIE_DOMAIN=hangoutsb.in
      - HCDA_OTEL_ENDPOINT=otel-collector:4317
    volumes:
      - ./core-services/hangout-content-delivery-api/certificates/private_key.pem:/mnt/certs/private_key.pem:ro
    depends_on:
      - otel-collector

  # * layer 3 components - depends on layer 0,1,2 components
  # kafka UI
  kafdrop:
    image: obsidiandynamics/kafdrop:4.1.1-SNAPSHOT
    deploy:
      replicas: 0
      resources:
        limits:
          cpus: 0.15
          memory: 350M
    ports:
      - "9004:9000"
    environment:
      - KAFKA_BROKERCONNECT=kafka:19091
      - SERVER_SERVLET_CONTEXTPATH=/
    depends_on:
      - kafka

  # authentication & authorization
  auth-api:
    image: ghcr.io/opticsquid/hangout-auth-api:v2.4.1
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.4
          memory: 300M
    environment:
      - DB_URL=geo-postgres:5432
      - DB_NAME=users
      - DB_USERNAME=postgres
      - DB_PASSWORD=${POSTGRES_PASSWORD}
      - KAFKA_SERVER=kafka:19091
      - ACCESS_TOKEN_SECRET=${AUTH_API_ACCESS_TOKEN_SECRET}
      - ACCESS_TOKEN_EXPIRY=300000
      - REFRESH_TOKEN_SECRET=${AUTH_API_REFRESH_TOKEN_SECRET}
      - REFRESH_TOKEN_LONG_TERM_EXPIRY=604800000
      - REFRESH_TOKEN_SHORT_TERM_EXPIRY=600000
      - NOTIFICATION_SERVICE=http://notification-service:5012
      - IP_API_URL=http://ip-api.com
      - OTEL_COLLECTOR=http://otel-collector:4317
      - LOG_DIR=/logs
      - INTERNAL_SERVICES_ORIGIN=http://notification-service:5012,http://post-api:5013
      - CLIENT_ORIGIN=http://localhost:5173/
      - COOKIE_DOMAIN=localhost
    volumes:
      - authApiLogs:/logs
    depends_on:
      - geo-postgres
      - otel-collector
      - kafka

  # sending out notifications through different channels to users. (excluding push notifications)
  notification-service:
    image: ghcr.io/opticsquid/hangout-notification-service:v1.0.5
    deploy:
      replicas: 0
      resources:
        limits:
          cpus: 0.1
          memory: 100M
    environment:
      - SMTP_SERVICE=gmail
      - SMTP_HOST=smtp.gmail.com
      - SMTP_PORT=465
      - SMTP_ISSECURE=true
      - SMTP_USERNAME=soumalyabhattacharya6@gmail.com
      - SMTP_PASSWORD=${NOTIFICATION_SERVICE_SMTP_PASSWORD}
      - MAIL_CONFIRMATION_URL=http://localhost:80/auth-api/v1/auth/verify?token=
      - JWT_PRIVATE_KEY=${NOTIFICATION_SERVICE_JWT_SECRET}
      - port=5012
      - KAFKA_SERVER=kafka:19091
      - OTEL_TRACES_EXPORTER=otlp
      - OTEL_LOGS_EXPORTER=otlp
      - OTEL_EXPORTER_OTLP_PROTOCOL=http
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    ports:
      - 5012:5012
    depends_on:
      - otel-collector
      - kafka

  # * layer 4 components - depends on layer 0,1,2,3 components
  # user content upload and fetch and content interactions
  post-api:
    image: ghcr.io/opticsquid/hangout-post-api:v1.5.9
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.4
          memory: 325M
    environment:
      - DB_URL=geo-postgres:5432
      - DB_NAME=posts
      - DB_USERNAME=postgres
      - DB_PASSWORD=${POSTGRES_PASSWORD}
      - KAFKA_SERVER=kafka:19091
      - OTEL_COLLECTOR=http://otel-collector:4317
      - AUTH_SERVICE=http://auth-api:5011/auth-api/v1
      - LOG_DIR=/logs
      - MINIO_SERVER=http://minio:9000
      - MINIO_ACCESS_KEY=sy22NKhjmHHCxGkSxn4h
      - MINIO_SECRET_KEY=${POST_API_MINIO_KEY}
      - UPLOAD_BUCKET=upload
      - CLIENT_ORIGINS=http://localhost:5173/
      - PAGE_LENGTH=25
      - FORMAT_SQL=true
      - DB_BATCH_SIZE=30
      - ORDER_INSERTS=true
      - ORDER_DELETES=true
      - GENERATE_DB_STATISTICS=true
      - CONTENT_TOPIC=content
      - HEART_TOPIC=hearts
      - MAX_POLL_RECS=100
      - MIN_BYTES=1024
      - MAX_WAIT_MS=10000
      - ADDRESS_API_URL=https://api.geoapify.com
      - ADDRESS_API_KEY=${POST_API_GEO_APIFY_KEY}
    volumes:
      - postApiLogs:/logs
    depends_on:
      - geo-postgres
      - minio
      - otel-collector
      - kafka
      - auth-api

  # user content processing
  storage-service:
    image: ghcr.io/opticsquid/hangout-storage-service:v2.1.5
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.4
          memory: 2G
    environment:
      - HSS_APPLICATION_NAME=hangout-storage-service
      - HSS_KAFKA_URL=kafka:19091
      - HSS_KAFKA_TOPIC=content
      - HSS_KAFKA_GROUP-ID=hangout-storage-service
      - HSS_LOG_LEVEL=debug
      - HSS_MINIO_BASE-URL=minio:9000
      - HSS_MINIO_ACCESS-KEY=ITtpzgn9Q0UjIbi0D71V
      - HSS_MINIO_SECRET-KEY=${STORAGE_SERVICE_MINIO_KEY}
      - HSS_MINIO_UPLOAD-BUCKET=upload
      - HSS_MINIO_STORAGE-BUCKET=processed
      - HSS_PROCESS_QUEUE-LENGTH=4
      - HSS_PROCESS_POOL-STRENGTH=4
      - HSS_DATASOURCE_USERNAME=postgres
      - HSS_DATASOURCE_PASSWORD=${POSTGRES_PASSWORD}
      - HSS_DATASOURCE_URL=geo-postgres:5432
      - HSS_DATASOURCE_DBNAME=posts
      - HSS_OTEL_ENDPOINT=otel-collector:4317
    depends_on:
      - geo-postgres
      - minio
      - otel-collector
      - cadvisor
      - kafka
      - post-api

  # user profile and activity
  profile-api:
    image: ghcr.io/opticsquid/hangout-profile-api:v1.1.0
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.4
          memory: 300M
    environment:
      - DB_URL=geo-postgres:5432
      - DB_NAME=profiles
      - DB_USERNAME=postgres
      - DB_PASSWORD=${POSTGRES_PASSWORD}
      - OTEL_COLLECTOR=http://otel-collector:4317
      - MINIO_SERVER=http://minio:9000
      - MINIO_ACCESS_KEY=tvX2CSzBYVYWjeewgefr
      - MINIO_SECRET_KEY=${PROFILE_API_MINIO_KEY}
      - UPLOAD_BUCKET=profile-photos
      - CLIENT_ORIGINS=http://localhost:5173/
      - FORMAT_SQL=true
      - AUTH_SERVICE=http://auth-api:5011
    depends_on:
      - geo-postgres
      - minio
      - otel-collector
      - kafka
      - auth-api

  # * layer 5 components - depends on layer 0,1,2,3,4 components
  # reverse-proxy for own services
  gateway:
    image: nginx:1.29.0-alpine
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.1
          memory: 50M
    ports:
      - 80:80
    volumes:
      - ./docker/gateway/nginx-compose.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - auth-api
      - post-api
      - minio
      - profile-api

  # * layer 6 components - depends on layer 0,1,2,3,4,5, components
  # frontend
  web:
    image: ghcr.io/opticsquid/hangout-web-new:v1.0.1
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: 0.1
          memory: 50M
    ports:
      - 5173:80
    depends_on:
      - gateway

volumes:
  geopostgresData:
    driver: local
  clickHouseData:
    driver: local
  clickHouseLogs:
    driver: local
  minioData:
    driver: local
  prometheusData:
    driver: local
  lokiData:
    driver: local
  tempoData:
    driver: local
  grafanaData:
    driver: local
  kafkaData:
    driver: local
  authApiLogs:
    driver: local
  postApiLogs:
    driver: local
